import os
import re
import time
import random
import requests
from bs4 import BeautifulSoup
import concurrent.futures
import img2pdf
from PIL import Image

def get_base_url(url):
    """æå–æ¼«ç”»åŸºç¡€URL"""
    match = re.match(r'^(https?://.*?/read-\d+)(?:-\d+)?/?$', url)
    return match.group(1) if match else url

def download_page(url, page_num, retries=5):
    """ä¸‹è½½å•ä¸ªé¡µé¢å›¾ç‰‡"""
    try:
        # éšæœºå»¶è¿Ÿï¼ˆ3-7ç§’ï¼‰
        time.sleep(random.uniform(3, 7))
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': url
        }
        
        with requests.Session() as session:
            # è·å–é¡µé¢å†…å®¹
            response = session.get(url, headers=headers)
            response.raise_for_status()
            
            # è§£æé¡µé¢å†…å®¹
            soup = BeautifulSoup(response.text, 'html.parser')
            p_tag = soup.find('p', class_='font-2 text-dark text-center mb-3')
            
            if not p_tag:
                raise ValueError("æœªæ‰¾åˆ°é¡µç æ ‡ç­¾")
            
            # è·å–å›¾ç‰‡æ ‡ç­¾
            img_tag = p_tag.find_next('img')
            if not img_tag or 'src' not in img_tag.attrs:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆå›¾ç‰‡æ ‡ç­¾")
            
            # ä¸‹è½½å›¾ç‰‡
            img_url = img_tag['src']
            img_response = session.get(img_url, headers=headers)
            img_response.raise_for_status()
            
            # ä¿å­˜å›¾ç‰‡
            filename = f"comic_images/{page_num:03d}.webp"
            with open(filename, 'wb') as f:
                f.write(img_response.content)
            
            print(f"âœ… æˆåŠŸä¸‹è½½ç¬¬ {page_num} é¡µ")
            return True
            
    except Exception as e:
        if retries > 0:
            print(f"âš ï¸ é‡è¯•ç¬¬ {page_num} é¡µï¼Œå‰©ä½™é‡è¯•æ¬¡æ•°ï¼š{retries}ï¼Œé”™è¯¯ï¼š{str(e)}")
            return download_page(url, page_num, retries-1)
        else:
            print(f"âŒ ç¬¬ {page_num} é¡µä¸‹è½½å¤±è´¥ï¼Œé”™è¯¯ï¼š{str(e)}")
            return False

def download_comic(initial_url, thread_num=5):
    """ä¸‹è½½æ¼«ç”»ä¸»å‡½æ•°"""
    # åˆ›å»ºå­˜å‚¨ç›®å½•
    os.makedirs('comic_images', exist_ok=True)
    
    # è·å–åŸºç¡€URL
    base_url = get_base_url(initial_url)
    
    # åˆå§‹åŒ–è¯·æ±‚è·å–æ€»é¡µæ•°
    with requests.Session() as session:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': initial_url
        }
        response = session.get(initial_url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # è§£æé¡µç ä¿¡æ¯
        p_tag = soup.find('p', class_='font-2 text-dark text-center mb-3')
        if not p_tag:
            raise ValueError("æ— æ³•å®šä½é¡µç ä¿¡æ¯")
        
        page_info = p_tag.b.text.strip().split('(')[-1].rstrip(')')
        current_page, total_pages = map(int, page_info.split('/'))
        
        # è·å–æ¼«ç”»åç§°
        img_tag = p_tag.find_next('img')
        comic_name = img_tag['alt'].strip() if img_tag else "æœªçŸ¥æ¼«ç”»"
    
    # ç”Ÿæˆæ‰€æœ‰é¡µé¢URL
    urls = [(base_url if page == 1 else f"{base_url}-{page}", page) 
            for page in range(current_page, total_pages + 1)]
    
    print(f"ğŸ“š å¼€å§‹ä¸‹è½½ã€Š{comic_name}ã€‹ï¼Œå…± {total_pages} é¡µ")
    
    # åˆ›å»ºçº¿ç¨‹æ± ä¸‹è½½
    with concurrent.futures.ThreadPoolExecutor(max_workers=thread_num) as executor:
        futures = [executor.submit(download_page, url, page) for url, page in urls]
        
        # æ˜¾ç¤ºè¿›åº¦
        for future in concurrent.futures.as_completed(futures):
            future.result()  # ä¿æŒå¼‚å¸¸ä¼ æ’­
    
    print("ğŸ‰ æ‰€æœ‰é¡µé¢ä¸‹è½½å®Œæˆ")
    return comic_name

def convert_to_pdf(comic_name):
    """è½¬æ¢ä¸ºPDFæ–‡ä»¶"""
    try:
        # è·å–æ’åºåçš„å›¾ç‰‡åˆ—è¡¨
        images = sorted([f for f in os.listdir('comic_images') if f.endswith('.webp')],
                       key=lambda x: int(x.split('.')[0]))
        
        # è½¬æ¢PDF
        with open(f"{comic_name}.pdf", "wb") as f:
            img_list = [Image.open(os.path.join('comic_images', img)) for img in images]
            pdf_bytes = img2pdf.convert([img.filename for img in img_list])
            f.write(pdf_bytes)
        
        print(f"âœ… æˆåŠŸç”ŸæˆPDFæ–‡ä»¶ï¼š{comic_name}.pdf")
        
    except Exception as e:
        print(f"âŒ PDFè½¬æ¢å¤±è´¥ï¼š{str(e)}")

if __name__ == "__main__":
    # é…ç½®å‚æ•°
    initial_url = "https://issmh.cc/read-3456"
    thread_num = 5  # å¯è°ƒèŠ‚çº¿ç¨‹æ•°
    
    try:
        # ä¸‹è½½æ¼«ç”»
        comic_name = download_comic(initial_url, thread_num)
        
        # è½¬æ¢PDF
        if comic_name:
            convert_to_pdf(comic_name)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        import shutil
        shutil.rmtree('comic_images')
        
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼š{str(e)}")